# -*- coding: utf-8 -*-
"""Ethelo_Topic_Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16WINKQ6G3w_V0xR0SbAP68oBnF1RGXL5

# **BERTopic - Using this for topic modeling!**
"""

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

!pip install bertopic[visualization] --quiet

"""# **Imports**"""

import numpy as np
import pandas as pd
from copy import deepcopy
from bertopic import BERTopic

"""# **Load data**"""

df = pd.read_csv("/content/gdrive/MyDrive/JaiShreeShyamJi@CMN_Professor_Frey_EthelloData (1)/Data sets/Okotoks 2021/comments.csv")

df.head(3)

docs = df.Content.values.tolist()

docs[:5]

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
print(stopwords.words('english'))
from nltk.tokenize import word_tokenize

for i in range(0, len(docs)):
  text_tokens = word_tokenize(docs[i])
  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]
  docs[i] = ' '.join(tokens_without_sw)

"""# **Creating Topics**"""

model = BERTopic(language="english")

docs

topics, probs = model.fit_transform(docs)

"""# We can then extract most frequent topics:"""

model.get_topic_freq()

"""# Get Individual Topics"""

model.get_topic(0)

model.get_topic(2)

for i in range(0,15):
  model.get_topic(i)

model.get_representative_docs()

"""# **Visualize Topics**"""

model.visualize_topics()

"""# **Clustering using GloVe embeddings**"""

!pip install -q sentence_transformers

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pprint import pprint

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

sentences = docs

sentence_embeddings = model.encode(sentences)

for sentence, embedding in zip(sentences, sentence_embeddings):
    print("Sentence:", sentence)
    print("Embedding:", embedding)
    print("")

len(sentence_embeddings)

len(sentence_embeddings[0])

dictionary = {}
for i in range(0, len(sentence_embeddings)):
  for j in range(i, len(sentence_embeddings)):
    dictionary[str(i) + "&" + str(j)] = cosine_similarity(sentence_embeddings[i].reshape(1, -1),
          sentence_embeddings[j].reshape(1, -1))[0][0]

sentences[0]

sentences[8]

sentences[2]

dictionary

